{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb New User Bookings\n",
    "## Student: Ruslan Kireev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "\"In this recruiting competition, Airbnb challenges you to predict in which country a new user will make his or her first booking\n",
    "\n",
    "You are given a list of users along with their demographics, web session records, and some summary statistics. You are asked to predict which country a new user's first booking destination will be. All the users in this dataset are from the USA.\n",
    "\n",
    "There are 12 possible outcomes of the destination country: 'US', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL','DE', 'AU', 'NDF' (no destination found), and 'other'. Please note that 'NDF' is different from 'other' because 'other' means there was a booking, but is to a country not included in the list, while 'NDF' means there wasn't a booking.\n",
    "\n",
    "The training and test sets are split by dates. In the test set, you will predict all the new users with first activities after 7/1/2014 (note: this is updated on 12/5/15 when the competition restarted). In the sessions dataset, the data only dates back to 1/1/2014, while the users dataset dates back to 2010. \"\n",
    "\n",
    "https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings/data\n",
    "\n",
    "The evaluation metric for this competition is NDCG (Normalized discounted cumulative gain) @k where k=5. NDCG is calculated as:\n",
    "\n",
    "$DCG_k=\\sum_{i=1}^k\\frac{2^{rel_i}-1}{\\log_2{\\left(i+1\\right)}},$ \n",
    "\n",
    "$nDCG_k=\\frac{DCG_k}{IDCG_k},$\n",
    "\n",
    "For example, if for a particular user the destination is FR, then the predictions become:\n",
    "\n",
    "[ FR ]  gives a $NDCG=\\frac{2^{1}-1}{log_{2}(1+1)}=1.0$\n",
    "\n",
    "[ US, FR ] gives a $DCG=\\frac{2^{0}-1}{log_{2}(1+1)}+\\frac{2^{1}-1}{log_{2}(2+1)}=\\frac{1}{1.58496}=0.6309$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset summary with basic statistics and respective plots + feature engineering\n",
    "\n",
    "Presented in the notebook called \"feature_engineering\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "Since this competition is over, we know the models which took places there. For example, the [3rd place](http://blog.kaggle.com/2016/03/07/airbnb-new-user-bookings-winners-interview-3rd-place-sandro-vega-pons/) used Gradient Boosting, Random Forest and Neural Networks, [2nd place](http://blog.kaggle.com/2016/03/17/airbnb-new-user-bookings-winners-interview-2nd-place-keiichi-kuroyanagi-keiku/) -- mostly Gradient Boosting. Also on forum there were mentions about good results of logistic regression. However, here we consider a solution based only on feature engineering and a single model. It's [shown](https://kaggle2.blob.core.windows.net/forum-message-attachments/108073/3717/ChallengeApproach_NabilAbdellaoui.pdf?sv=2012-02-12&se=2016-07-01T14%3A01%3A30Z&sr=b&sp=r&sig=GLpko2ZxgWmQ5mM0SfWSsAt0Tbxf7bC9HIaa24yAHZo%3D) that sophisticated multi-layer ensemble architecture give only about +0.1% benefit versus a single model. In this project we concentrate on Gradient Boosting and compare with mentioned above methods: logistic regression and random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup and results\n",
    "\n",
    "For tuning parameters of the GB model we use randomized search (I have not much time for grid search:/) and the best result we get with {'colsample_bytree': 0.4, 'learning_rate': 0.1, 'n_estimators': 10, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 10} which gives mean: 0.84270, std: 0.00662 in 3 fold CV. \n",
    "\n",
    "Comparing to the other models: Random Forest without any tuning gives mean: 0.84933, Logistic Regression with l1 regularization and C=0.03125 gives mean: 0.84863, kNN with 3000 neighbors -- 0.82."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "train = pd.read_csv('train_preprocessed.csv') # file generated by feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import grid_search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.random.seed(0)\n",
    "del train['Unnamed: 0']\n",
    "del train['id']\n",
    "y = train['country_destination'].copy()\n",
    "del train['country_destination']\n",
    "X = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73815, 144)\n"
     ]
    }
   ],
   "source": [
    "X = pd.get_dummies(X)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dcg_score(y_true, y_score, k=5):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(ground_truth, predictions, k=5):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(range(predictions.shape[1] + 1))\n",
    "    T = lb.transform(ground_truth)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Iterate over each y_true and compute the DCG score\n",
    "    for y_true, y_score in zip(T, predictions):\n",
    "        actual = dcg_score(y_true, y_score, k)\n",
    "        best = dcg_score(y_true, y_true, k)\n",
    "        score = float(actual) / 1.0#float(best)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# NDCG Scorer function\n",
    "ndcg_scorer = make_scorer(ndcg_score, needs_proba=True, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6574256 ,  0.65905852,  0.65953264])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dumb = DummyClassifier()\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "cross_validation.cross_val_score(dumb, X, y, cv=3, scoring=ndcg_scorer)\n",
    "#dummy_estimator_predictions = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.852841665288\n",
      "0.845922791813\n",
      "0.849347468133\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=250)#, max_features=10)\n",
    "kf = cross_validation.KFold(len(X), n_folds=3)#, random_state=42)\n",
    "for train_index, test_index in kf:\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            #y_pred = clf.predict(X_test)\n",
    "            y_prob = clf.predict_proba(X_test)\n",
    "            print ndcg_score(y_test, y_prob, k=5)\n",
    "#print cross_validation.cross_val_score(clf, X, y, cv=3, scoring=ndcg_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "          fit_params={}, iid=True, n_iter=20, n_jobs=1,\n",
       "          param_distributions={'n_estimators': [10, 50, 100], 'subsample': [0.3, 1], 'seed': [0], 'colsample_bytree': [0.3, 0.4, 0.5], 'objective': ['multi:softprob'], 'learning_rate': [0.1, 0.3, 0.5, 0.7, 0.9], 'max_depth': [5, 7, 10, 20], 'nthread': [4]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring=make_scorer(ndcg_score, needs_proba=True, k=5),\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dist = {\"max_depth\": [5,7,10,20],\n",
    "              \"learning_rate\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "              \"n_estimators\": [10, 50, 100],\n",
    "              \"objective\": ['multi:softprob'],\n",
    "              \"subsample\": [0.3, 1],\n",
    "              \"colsample_bytree\": [0.3, 0.4, 0.5],\n",
    "              \"seed\":[0],\n",
    "              \"nthread\":[4]}\n",
    "xgb = XGBClassifier()\n",
    "random_search = grid_search.RandomizedSearchCV(xgb, param_distributions=param_dist, n_iter=20, scoring=ndcg_scorer)\n",
    "random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.83781, std: 0.00951, params: {'colsample_bytree': 0.3, 'learning_rate': 0.1, 'nthread': 4, 'n_estimators': 100, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 5},\n",
       " mean: 0.80987, std: 0.00860, params: {'colsample_bytree': 0.3, 'learning_rate': 0.3, 'nthread': 4, 'n_estimators': 100, 'subsample': 1, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 10},\n",
       " mean: 0.78016, std: 0.02128, params: {'colsample_bytree': 0.3, 'learning_rate': 0.7, 'nthread': 4, 'n_estimators': 50, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 5},\n",
       " mean: 0.79215, std: 0.00818, params: {'colsample_bytree': 0.4, 'learning_rate': 0.5, 'nthread': 4, 'n_estimators': 50, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 20},\n",
       " mean: 0.74729, std: 0.02693, params: {'colsample_bytree': 0.5, 'learning_rate': 0.9, 'nthread': 4, 'n_estimators': 10, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 10},\n",
       " mean: 0.81232, std: 0.01270, params: {'colsample_bytree': 0.3, 'learning_rate': 0.7, 'nthread': 4, 'n_estimators': 10, 'subsample': 1, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 20},\n",
       " mean: 0.73968, std: 0.02156, params: {'colsample_bytree': 0.5, 'learning_rate': 0.9, 'nthread': 4, 'n_estimators': 100, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 7},\n",
       " mean: 0.77836, std: 0.01957, params: {'colsample_bytree': 0.5, 'learning_rate': 0.7, 'nthread': 4, 'n_estimators': 100, 'subsample': 1, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 20},\n",
       " mean: 0.75010, std: 0.01245, params: {'colsample_bytree': 0.4, 'learning_rate': 0.9, 'nthread': 4, 'n_estimators': 50, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 20},\n",
       " mean: 0.81486, std: 0.00478, params: {'colsample_bytree': 0.3, 'learning_rate': 0.3, 'nthread': 4, 'n_estimators': 50, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 20},\n",
       " mean: 0.76430, std: 0.01721, params: {'colsample_bytree': 0.5, 'learning_rate': 0.5, 'nthread': 4, 'n_estimators': 50, 'subsample': 1, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 7},\n",
       " mean: 0.84270, std: 0.00662, params: {'colsample_bytree': 0.4, 'learning_rate': 0.1, 'nthread': 4, 'n_estimators': 10, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 10},\n",
       " mean: 0.82654, std: 0.01373, params: {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'nthread': 4, 'n_estimators': 100, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 20},\n",
       " mean: 0.76471, std: 0.02531, params: {'colsample_bytree': 0.5, 'learning_rate': 0.3, 'nthread': 4, 'n_estimators': 100, 'subsample': 1, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 5},\n",
       " mean: 0.82045, std: 0.02020, params: {'colsample_bytree': 0.5, 'learning_rate': 0.5, 'nthread': 4, 'n_estimators': 10, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 5},\n",
       " mean: 0.83852, std: 0.01108, params: {'colsample_bytree': 0.3, 'learning_rate': 0.1, 'nthread': 4, 'n_estimators': 50, 'subsample': 1, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 5},\n",
       " mean: 0.81513, std: 0.03669, params: {'colsample_bytree': 0.4, 'learning_rate': 0.5, 'nthread': 4, 'n_estimators': 10, 'subsample': 1, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 5},\n",
       " mean: 0.77113, std: 0.00621, params: {'colsample_bytree': 0.5, 'learning_rate': 0.7, 'nthread': 4, 'n_estimators': 50, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 7},\n",
       " mean: 0.79210, std: 0.02393, params: {'colsample_bytree': 0.5, 'learning_rate': 0.5, 'nthread': 4, 'n_estimators': 50, 'subsample': 1, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 20},\n",
       " mean: 0.83523, std: 0.00601, params: {'colsample_bytree': 0.3, 'learning_rate': 0.1, 'nthread': 4, 'n_estimators': 100, 'subsample': 0.3, 'seed': 0, 'objective': 'multi:softprob', 'max_depth': 10}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "100 [ 0.81544399  0.81523815  0.81469264]\n",
      "1000 [ 0.81966987  0.81947392  0.82004528]\n",
      "3000 [ 0.82012622  0.82006935  0.82015512]\n",
      "LGT\n",
      "l1 0.0078125 [ 0.85090392  0.84621748  0.84684889]\n",
      "l2 0.0078125 [ 0.82376356  0.82242463  0.82571722]\n",
      "l1 0.03125 [ 0.85152867  0.84763801  0.84683589]\n",
      "l2 0.03125 [ 0.8214212   0.82157342  0.82527776]\n",
      "l1 0.125 [ 0.85021309  0.84678948  0.84616267]\n",
      "l2 0.125 [ 0.82254001  0.82157342  0.82320196]\n",
      "l1 0.5 [ 0.84948908  0.84639968  0.83985412]\n",
      "l2 0.5 [ 0.82245439  0.82322436  0.82317195]\n",
      "l1 2 [ 0.84947697  0.84614516  0.83357781]\n",
      "l2 2 [ 0.82254001  0.82326936  0.82300692]\n",
      "l1 8 [ 0.8190911   0.84585896  0.82068409]\n",
      "l2 8 [ 0.82254001  0.82157164  0.82441724]\n"
     ]
    }
   ],
   "source": [
    "print 'KNN' \n",
    "for k in [100, 1000, 3000]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    print k, cross_validation.cross_val_score(knn, X, y, cv=3, scoring=ndcg_scorer)\n",
    "print 'LGT'\n",
    "for c in range(-7, 5, 2):\n",
    "    for l in ['l1', 'l2']:\n",
    "        lgt = LogisticRegression(penalty=l, C=2**c)\n",
    "        print l, 2**c, cross_validation.cross_val_score(lgt, X, y, cv=3, scoring=ndcg_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Discussion\n",
    "\n",
    "If we look on the Leader Board we will see scores much higher than here. However, it's a normal thing the LB score differs from local CV in this competition. [Look](http://blog.kaggle.com/2016/03/17/airbnb-new-user-bookings-winners-interview-2nd-place-keiichi-kuroyanagi-keiku/):\n",
    "<img src=\"lbcv.png\">\n",
    "\n",
    "So, why did Random forest and logistic regression beat Gradient boosting? First of all, there is a need for a more sensitive grid search. Second, we don't know yet if this realy loses to RF and LR until we submit it. \n",
    "\n",
    "Ideally, we also need to perform the feature selection step and play around with ensamble methods and further feature engineering, investigate feature importance, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Besides, a dummy classifier based of frequences gives 0.6598, which corresponds to Sample Submission Benchmark of 0.6791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
